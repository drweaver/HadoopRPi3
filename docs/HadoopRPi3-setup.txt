Raspberry Pi Hadoop Cluster Configuration Notes

These notes are based on blogs by Jonas Widriksson and my own thoughts and experiences.
This is running on the RPi3 Model B.

This first version of the notes sets up just the first node and gets to the point where it can run wordcount jobs.

The initial blog was for Hadoop V1.2.1 - that blog contains a good overview of the components of Hadoop in V1.2.1:
http://www.widriksson.com/raspberry-Pi-Hadoop-cluster/

The later blog for Hadoop 2.7.2 contains good information about the differences in Hadoop V2,
and setup detail that works for V2.7.2:
http://www.widriksson.com/raspberry-pi-2-hadoop-2-cluster/

Some simple scripts are available in a GitHub repo. The scripts have groups of commands from the steps below for convenience.
Look in:
https://github.com/phil-davis/HadoopRPi3/tree/master/scripts

Installation Steps
==================
01) Install RPi3s with default Raspbian OS

To see what version you have:
pi@RPi3101:~ $ cat /etc/os-release
	PRETTY_NAME="Raspbian GNU/Linux 8 (jessie)"
	NAME="Raspbian GNU/Linux"
	VERSION_ID="8"
	VERSION="8 (jessie)"
	ID=raspbian
	ID_LIKE=debian
	HOME_URL="http://www.raspbian.org/"
	SUPPORT_URL="http://www.raspbian.org/RaspbianForums"
	BUG_REPORT_URL="http://www.raspbian.org/RaspbianBugs"

To see the Linux version:
pi@RPi3101:~ $ cat /proc/version
	Linux version 4.1.19-v7+ (dc4@dc4-XPS13-9333) (gcc version 4.9.3 (crosstool-NG crosstool-ng-1.22.0-88-g8460611) ) #858 SMP Tue Mar 15 15:56:00 GMT 2016

Get updates whenever you like:
	sudo apt-get update

And any other utilities:
	sudo apt-get install zip oracle-java7-jdk rpi-update

Run rasp-config:
- enable SSH service
- set memory split to 16Mb in advanced settings
- set time and localisation
- set hostname pi0 to pi3

02) Network setup

Attach wired network to each Pi. I am using RJ45 network cables up to a switch.

Setup dhcp on the head node on the eth0 interface:

	sudo apt-get install isc-dhcp-server
	# copy across /etc/dhcp/dhcp.conf file <- setup for dhcp and static assignments
	# copy across /etc/default/isc-dhcp-server file <- enables dhcp on eth0
	# copy across /etc/iptables.ipv4.nat <- this allows routing via head node
	# copy across /etc/network/interfaces (remove pi0 extension)
	# copy across /etc/wpa_supplicant/wpa_supplicant.conf <- wlan0 (wifi setup) edit with your ssid/pwd
	sudo service isc-dhcp-server start
	# might be worth rebooting at this point
	sudo reboot

On slave nodes:
	# copy across /etc/network/interfaces (remove pi1-3 extension)
	sudo reboot


03) Make sure routing is working in cluster
at this point should be able to connect to head node via home wifi (need to check your router for it’s IP address).
From pi0, can ssh to pi1, pi2, pi3. 
Make sure pi0 is acting as a router. When logged into pi1 (192.168.50.11) try:
	ssh pi@[ip address of pi0 connected via wifi to home network]
	ssh pi@192.168.50.11
	ping www.google.com
	sudo apt-get update
	sudo apt-get upgrade


04) Edit /etc/hosts and put the IP addresses and names in, to make life easy and not depend on having a DNS server:
	# copy across /etc/hosts file or past at the end:
	192.168.50.10 pi0 pi0.local pi0.lan pi0.cluster
	192.168.50.11 pi1 pi1.local pi1.lan pi1.cluster
	192.168.50.12 pi2 pi2.local pi2.lan pi2.cluster
	192.168.50.13 pi3 pi3.local pi3.lan pi3.cluster

05) Format the USB stick ready for use:
	sudo mkfs.ext4 /dev/sda1

06) Ensure the usb stick is mounted at boot:
	# copy across /etc/fstab from back or paste at the bottom:
	/dev/sda1 /mnt/usb ext4 defaults,user,auto 0 1

07) Test you can mount the directory
	sudo mount /mnt/usb
	sudo mkdir /mnt/usb/hdfs

08) Change the password for the "pi" username (use the passwd command) - if you want to!

09) Check that Java is installed:
	pi@pi0:~ $ java -version
	java version "1.7.0_60"
	Java(TM) SE Runtime Environment (build 1.7.0_60-b19)
	Java HotSpot(TM) Client VM (build 24.60-b09, mixed mode)

if not there, install from step 1)

10) Create the hadoop group and hduser user (createhduser.sh):
	sudo addgroup hadoop
	sudo adduser --ingroup hadoop hduser
		(enter password for user and default info as asked)
	sudo adduser hduser sudo
		User hduser is now in the hadoop and sudo groups.

11) Create ssh rsa pair keys (createsshkeys.sh):
Login to a shell as hduser (e.g. "su hduser") and:
	mkdir ~/.ssh
	ssh-keygen -t rsa -P ""
	cat ~/.ssh/id_rsa.pub > ~/.ssh/authorized_keys

	# now copy this key to all other pis
	ssh-copy-id pi1
	# same for pi2 and pi3	

When setting up a cluster, do not create new ones of these on each node.
Use the ones from the main (1st) node.
That way hduser on each of the nodes will autologin from the others.

Then test that it works (say yes the first time if you are prompted about the new keys):
	ssh localhost
to make sure you can login without giving a password.
	ssh pi1
	exit
	# do the same for pi2 and pi3

12) Download and install the latest stable hadoop (gethadoop.sh):
	wget http://apache.mirrors.spacedump.net/hadoop/core/stable/hadoop-2.7.2.tar.gz
	sudo mkdir /opt (note - the dir is already there, so this can be skipped)
	sudo tar -xvzf hadoop-2.7.2.tar.gz -C /opt/
	sudo mv /opt/hadoop-2.7.2 /opt/hadoop
	sudo chown -R hduser:hadoop /opt/hadoop

13) Hadoop spits out messages like:
Java HotSpot(TM) Client VM warning: You have loaded library /opt/hadoop/lib/native/libhadoop.so.1.0.0 which might have disabled stack guard. The VM will try to fix the stack guard now.
It's highly recommended that you fix the library with 'execstack -c <libfile>', or link it with '-z noexecstack'.

	# fixed by copying file /opt/hadoop/etc/hadoop/hadoop-env.sh over (copy them all across)
	# the addition of -XX:-PrintWarnings fixes this in HADOOP_OPTS

14) Configure environment variables for Hadoop stuff by adding to the end of /etc/bash.bashrc
(e.g. edit with "sudo nano /etc/bash.bashrc") or just copy across from backup dir
	export JAVA_HOME=$(readlink -f /usr/bin/java | sed "s:jre/bin/java::")
	export HADOOP_INSTALL=/opt/hadoop
	export PATH=$PATH:$HADOOP_INSTALL/bin
	export PATH=$PATH:$HADOOP_INSTALL/sbin
	export HADOOP_MAPRED_HOME=$HADOOP_INSTALL
	export HADOOP_COMMON_HOME=$HADOOP_INSTALL
	export HADOOP_HDFS_HOME=$HADOOP_INSTALL
	export YARN_HOME=$HADOOP_INSTALL
	export HADOOP_HOME=$HADOOP_INSTALL

15) Setup some other Hadoop-specific environment variables in
	/opt/hadoop/etc/hadoop/hadoop-env.sh <- copy across from backup or do the below

See the version on GitHub. Currently the only change to the supplied file is the JAVA_HOME line:
	export JAVA_HOME=$(readlink -f /usr/bin/java | sed "s:bin/java::")

16) Check that it works by running hadoop from hduser:
	su hduser
	hduser@RPi3101:/home/pi $ hadoop version
	Hadoop 2.7.2
	Subversion https://git-wip-us.apache.org/repos/asf/hadoop.git -r b165c4fe8a74265c792ce23f546c64604acf0e41
	Compiled by jenkins on 2016-01-26T00:08Z
	Compiled with protoc 2.5.0
	From source with checksum d0fda26633fa762bff87ec759ebe689c
	This command was run using /opt/hadoop/share/hadoop/common/hadoop-common-2.7.2.jar

	exit ! fron hduser

17) In /opt/hadoop/etc/hadoop put the following (copy across the backup files):
- core-site.xml
- hdfs-site.xml  <— set the dfs.replication value to the number of nodes in the cluster.
- mapred-site.xml
- yarn-site.xml
- slaves <- list the nodes in the cluster, should always include head node: pi0

18) Create the HDFS file system (createhdfs.sh):
	sudo mkdir -p /mnt/usb/hdfs
	sudo chown hduser:hadoop /mnt/usb/hdfs

And format HDFS from hduser:
	hdfs namenode -format

19) Start services from hduser:
	start-dfs.sh
	start-yarn.sh

To stop (as and when needed):
	stop-dfs.sh
	stop-yarn.sh

20) See what stuff is running:
hduser$ jps
30192 DataNode
30531 Jps
28983 ResourceManager
30394 SecondaryNameNode
29452 NodeManager
30077 NameNode

You should see those 6 things above, in some order and with different PID numbers.

21) Get some sample text files to use as data input for wordcount (gettextfiles.sh):
	# Make a textfiles sub-directory if it does not exist
	mkdir -p textfiles
	cd ./textfiles

	# Get and unpack smallfile.txt (about 1.2MB) and mediumfile.txt (about 35MB) (thanks to widriksson site)
	# These are handy for demonstrating wordcount
	# Note: See http://www.widriksson.com/raspberry-pi-2-hadoop-2-cluster/ for a good tutorial of setting up an RPi Hadoop cluster
	wget http://www.widriksson.com/wp-content/uploads/2014/10/hadoop_sample_txtfiles.tar.gz
	tar -xf hadoop_sample_txtfiles.tar.gz

	# Get and unpack the text of some other famous books available from the Gutenberg project
	# Ulysses by James Joyce
	wget http://www.gutenberg.org/files/4300/4300.zip
	unzip 4300.zip

	# Adventures of Huckleberry Finn by Mark Twain
	wget http://www.gutenberg.org/files/76/76.zip
	unzip 76.zip

	# War and Peace by Leo Tolstoy
	wget http://www.gutenberg.org/files/2600/2600.zip
	unzip 2600.zip

	# eof

The textfiles dir ends up with:
	-rw-r--r-- 1 hduser hadoop  3226651 May  6 16:04 2600.txt
	-rw-r--r-- 1 hduser hadoop  1199841 May 27 03:36 2600.zip
	-rw-r--r-- 1 hduser hadoop  1573079 Dec  9  2014 4300.txt
	-rw-r--r-- 1 hduser hadoop   661646 May 27 03:36 4300.zip
	-rw-r--r-- 1 hduser hadoop   606661 Apr 18  2015 76.txt
	-rw-r--r-- 1 hduser hadoop   226068 May 27 03:36 76.zip
	-rw-r--r-- 1 hduser hadoop 14157806 Oct  9  2014 hadoop_sample_txtfiles.tar.gz
	-rw-r--r-- 1 hduser hadoop 35926176 Oct  6  2014 mediumfile.txt
	-rw-r--r-- 1 hduser hadoop  1226438 Oct  6  2014 smallfile.txt

22) Run a small wordcount job to make sure things are good:
	# Copy all the *.txt files from the textfiles dir under the user home dir
	# into a textfiles dir in the Hadoop Distributed File System
	hadoop fs -mkdir /textfiles
	hadoop fs -put ~/textfiles/*.txt /textfiles
	hadoop fs -ls /textfiles

	# Run a wordcount on smallfile.txt
	# Timing information will appear at the end of the output, it takes about 1 minute 10 seconds on my RPi3
	time hadoop jar /opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.2.jar wordcount /textfiles/smallfile.txt /smallfile-out
	# Or try a medium-sized job
	time hadoop jar /opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.2.jar wordcount /textfiles/mediumfile.txt /mediumfile-out

	# Get the smallfile-out output folder from HDFS into a local dir
	hadoop fs -get /smallfile-out ./smallfile-out

	# Have a look at the counts of words
	more ./smallfile-out/part-r-00000

There are other example programs like:
	wordcount
	wordmean
	wordmedian
	wordstandarddeviation

You can see what is provided in the example code with the command:
	hadoop jar /opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.2.jar

To expand the network to include other RPi3 systems:

On the 1st (main) node:
a) Clear out the HDFS filesystem (from hduser on every node):
	rm -rf /mnt/usb/hdfs/*
	hdfs namenode -format

b) Put all the systems into /opt/hadoop/etc/hadoop/slaves 
	pi0
	pi1
	pi2
	pi3

c) core-site.xml - no change needed, i.e. fs.defaultFS points to the main node.

d) hdfs-site.xml - change replication to 4 (or however many nodes you will have)
<property>
<name>dfs.replication</name>
<value>4</value>
</property>

e) yarn-site.xml - no change needed, i.e. the address parameters all point to the main node.

f) If you didn’t create slaves manually, you can clone the whole pi0 MicroSD card, make sure to set /etc/hostname individually on each clone.
If you setup each node individually from the single-node instructions,
then make sure that the items (a) to (e) are done on each node, and
copy the files from hduser .ssh folder on the 1st node to hduser .ssh folder on the other nodes
(rather than doing step 11 above)

g) As hduser on the main node, start DFS and YARN:
(This will start the needed processes on the slaves also)

start-dfs.sh
start-yarn.sh

h) From hduser on each node, check that processes are running:
On the main node:
hduser@RPi3101:~ $ jps
3488 NameNode
4145 NodeManager
3810 SecondaryNameNode
4022 ResourceManager
4364 Jps
3615 DataNode

On the slaves:
hduser@RPi3102:~ $ jps
19606 NodeManager
19486 DataNode
19743 Jps

hduser@RPi3103:~ $ jps
19331 Jps
19192 NodeManager
19071 DataNode

g) Run tests of wordcount from the main node, as in step 22,
copy the txt files into HDFS
then run the wordcount

See the status of the cluster, executing jobs and finished jobs with your browser at port 8088 of the main node e.g.:
http://[wifi ip address of pi0]:8088/cluster
